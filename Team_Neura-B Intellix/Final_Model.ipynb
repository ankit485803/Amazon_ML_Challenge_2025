{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180cbb38-c32d-490c-b989-fe5468b64323",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 1: Load Data (train and test CSV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81969ac3-3db2-4c7c-a16d-40faab0713e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3      55858  Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...   \n",
      "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
      "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
      "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \n",
      "   sample_id                                    catalog_content  \\\n",
      "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
      "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
      "2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
      "3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
      "4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n",
      "\n",
      "                                          image_link  \n",
      "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
      "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n",
      "2  https://m.media-amazon.com/images/I/61KCM61J8e...  \n",
      "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...  \n",
      "4  https://m.media-amazon.com/images/I/71QYlrOMoS...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your files\n",
    "train_path = 'C:/Users/sanja/Desktop/Amazon/data/train.csv'\n",
    "test_path = 'C:/Users/sanja/Desktop/Amazon/data/test.csv'\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e171c-813b-45d7-ab10-af01c27d7edf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "# Step 2: Build Custom PyTorch Dataset to Fetch Images on-the-fly\n",
    "\n",
    "This Dataset will:\n",
    "\n",
    "* Read image URLs from the dataframe\n",
    "* Download and transform images on demand\n",
    "* Process catalog_content into text tokens/embeddings\n",
    "* Return combined features and target (for train)\n",
    "\n",
    "Note: We prefer the on-the-fly fetching approach from URLs during training rather than downloading the complete dataset on the PC\n",
    "A custom PyTorch Dataset fetching images dynamically during training and optionally caching them is efficient and scalable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a794f2ca-8fd2-4c0e-a0e1-3b9613d63f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import AutoTokenizer  # tokenizer\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128, train=True, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Text processing\n",
    "        text = row['catalog_content']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Image fetching & processing (no caching)\n",
    "        image_url = row['image_link']\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=5)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        except:\n",
    "            # Use placeholder on failure\n",
    "            img = Image.new('RGB', (224, 224), color='gray')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            price = torch.tensor(row['price'], dtype=torch.float)\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'image': img,\n",
    "                'price': price\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'image': img,\n",
    "                'sample_id': row['sample_id']\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9f2cb-8f27-4570-bdbc-850810f3b080",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 3: Model Building (Multi-Modal: Text + Image)\n",
    "\n",
    "We'll use:\n",
    "\n",
    "* A pretrained DistilBERT model to extract text embeddings.\n",
    "* A pretrained CNN (e.g., ResNet18) to extract image embeddings.\n",
    "* Concatenate the two feature vectors.\n",
    "* Feed them into fully connected layers for regression output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f4dcfc-cd9c-49bd-942d-b981622ae800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "\n",
    "class MultiModalPricePredictor(nn.Module):\n",
    "    def __init__(self, text_model_name='distilbert-base-uncased', cnn_model_name='resnet18', cnn_pretrained=True, fine_tune_text=False):\n",
    "        super(MultiModalPricePredictor, self).__init__()\n",
    "\n",
    "        # Text model - DistilBERT\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        if not fine_tune_text:\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        text_embedding_dim = self.text_model.config.hidden_size  # 768 for DistilBERT\n",
    "\n",
    "\n",
    "        \n",
    "        # Image model - ResNet18 feature extractor\n",
    "        \n",
    "        # cnn = resnet18(pretrained=cnn_pretrained)\n",
    "        cnn = resnet18(weights=ResNet18_Weights.DEFAULT if cnn_pretrained else None)\n",
    "        modules = list(cnn.children())[:-1]  # Remove last FC layer\n",
    "        self.cnn_model = nn.Sequential(*modules)\n",
    "        for param in self.cnn_model.parameters():\n",
    "            param.requires_grad = fine_tune_text  # Optionally freeze CNN layers\n",
    "\n",
    "        image_embedding_dim = 512  # ResNet18 last conv layer output\n",
    "\n",
    "        # Combine embeddings and output regression\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim + image_embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)  # Output single float (price)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Text embeddings (use CLS token representation)\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeds = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Image embeddings\n",
    "        #img_embeds = self.cnn_model(image).squeeze()  # [batch, 512, 1, 1] -> [batch, 512]\n",
    "        img_embeds = torch.flatten(self.cnn_model(image), 1)  # Flatten the [batch, 512, 1, 1] tensor to [batch, 512]\n",
    "\n",
    "\n",
    "        # Concatenate\n",
    "        combined = torch.cat((text_embeds, img_embeds), dim=1)\n",
    "        output = self.fc(combined).squeeze(1)  # Output shape [batch]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d88b2e-749c-4977-bed1-88a53b4ccfdd",
   "metadata": {},
   "source": [
    "# Step 4: Training Loop Setup\n",
    "\n",
    "We will build a PyTorch training loop using an optimizer and scheduler. Use MSE loss for regression and calculate SMAPE for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5aaa5df-23e5-4c97-9e06-e01a38867808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def smape(y_true, y_pred, epsilon=1e-6):\n",
    "    numerator = torch.abs(y_pred - y_true)\n",
    "    denominator = (torch.abs(y_pred) + torch.abs(y_true) + epsilon) / 2.0\n",
    "    return torch.mean(numerator / denominator)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=3, lr=2e-5, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            prices = batch['price'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(preds, prices)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_smapes = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                prices = batch['price'].to(device)\n",
    "\n",
    "                preds = model(input_ids, attention_mask, images)\n",
    "                loss = criterion(preds, prices)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                score = smape(prices, preds)\n",
    "                val_smapes.append(score.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_smape = sum(val_smapes) / len(val_smapes)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val SMAPE={avg_val_smape:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c23c4-6acd-484a-b78e-2b4ee808ebfb",
   "metadata": {},
   "source": [
    "# Step 5: Data Loaders and Training Execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679fa17-2dc8-4552-9a06-f7e8f6fa6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up tokenizer (DistilBERT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Set up image transforms (for ResNet and similar CNNs)\n",
    "    image_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load CSV data\n",
    "    train_df = pd.read_csv(\"C:/Users/sanja/Desktop/Amazon/data/train.csv\")\n",
    "    test_df = pd.read_csv(\"C:/Users/sanja/Desktop/Amazon/data/test.csv\")\n",
    "\n",
    "    # Set up your dataset\n",
    "    full_dataset = ProductDataset(df=train_df, tokenizer=tokenizer, transform=image_transforms, train=True)\n",
    "\n",
    "    # Train-validation split\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # DataLoaders with num_workers=0 for Windows/Jupyter\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize model and device\n",
    "    model = MultiModalPricePredictor()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Train your model\n",
    "    trained_model = train_model(model, train_loader, val_loader, epochs=3, device=device)\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate the time taken for training\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time  # Time in seconds\n",
    "\n",
    "    # Convert total time to minutes and seconds\n",
    "    minutes = total_time // 60\n",
    "    seconds = total_time % 60\n",
    "    \n",
    "\n",
    "    # Print training time in minutes and seconds\n",
    "    print(\"Training completed\")\n",
    "    print(f\"Total training time: {minutes:.0f} minutes and {seconds:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6535a-d033-49d1-8392-a632a354bf0b",
   "metadata": {},
   "source": [
    "#  Step 6. Validation Accuracy and Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec97d11-46a0-4cb8-a5e5-ac05f9f23384",
   "metadata": {},
   "source": [
    "##  6.1 : Calculate SMAPE on Validation Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52134f-7da0-4689-836e-0a57afc4eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred, epsilon=1e-6):\n",
    "    numerator = torch.abs(y_pred - y_true)\n",
    "    denominator = (torch.abs(y_pred) + torch.abs(y_true) + epsilon) / 2.0\n",
    "    return torch.mean(numerator / denominator).item()\n",
    "\n",
    "# After training, run evaluation:\n",
    "model.eval()\n",
    "smape_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        prices = batch['price'].to(device)\n",
    "\n",
    "        preds = model(input_ids, attention_mask, images)\n",
    "        smape_list.append(smape(prices, preds))\n",
    "\n",
    "final_val_smape = sum(smape_list) / len(smape_list)\n",
    "print(f\"Validation SMAPE: {final_val_smape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ecc6d-5ec2-463a-b946-f2d94188bac5",
   "metadata": {},
   "source": [
    "## 6.2 Generate Predictions for Test Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0daca3c-bc4a-41e0-87b4-5785e786806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset (no price column)\n",
    "test_dataset = ProductDataset(df=test_df, tokenizer=tokenizer, transform=image_transforms, train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "sample_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        preds = model(input_ids, attention_mask, images)\n",
    "        \n",
    "        # Ensure predicted prices are positive\n",
    "        preds = torch.clamp(preds, min=0.01)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        sample_ids.extend(batch['sample_id'].cpu().numpy())\n",
    "\n",
    "# Final export as a CSV\n",
    "import pandas as pd\n",
    "output_df = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'price': [float(p) for p in predictions]\n",
    "})\n",
    "output_df.to_csv('test_out.csv', index=False)\n",
    "print(\"Exported predictions to test_out.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a3494-bace-4bf6-90e9-b27bc8a38b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02815bf-9416-445c-a631-ca204a80d9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
